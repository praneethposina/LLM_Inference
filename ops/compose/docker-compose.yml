# LLM Inference Stack - Docker Compose
#
# This brings up the complete inference stack:
#   - gateway: OpenAI-compatible API (port 8000)
#   - vllm: LLM inference engine (internal port 8001)
#
# Usage:
#   docker compose up              # Start with default model (Mistral 7B)
#   docker compose up -d           # Start in background
#   docker compose down            # Stop all services
#
# Switch models:
#   MODEL=google/gemma-2-9b-it docker compose up
#
# View logs:
#   docker compose logs -f vllm    # vLLM logs (model loading progress)
#   docker compose logs -f gateway # Gateway logs (requests)

services:
  # ===========================================================================
  # vLLM - LLM Inference Engine
  # ===========================================================================
  # Runs the actual model inference with:
  #   - Continuous batching
  #   - PagedAttention for efficient KV cache
  #   - OpenAI-compatible API
  vllm:
    build:
      context: ../../engines/vllm
      dockerfile: Dockerfile
    container_name: llm-vllm
    environment:
      # Model selection - override with MODEL=xxx docker compose up
      - MODEL=${MODEL:-mistralai/Mistral-7B-Instruct-v0.3}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-8192}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.90}
      # HuggingFace cache - mount below to persist model downloads
      - HF_HOME=/root/.cache/huggingface
    volumes:
      # Persist model downloads across container restarts
      - huggingface_cache:/root/.cache/huggingface
    # GPU access - requires nvidia-container-toolkit
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    ports:
      - "8001:8000" # Expose for debugging; gateway uses internal network
    networks:
      - llm-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s # Models take time to load
    restart: unless-stopped

  # ===========================================================================
  # Gateway - OpenAI-compatible API
  # ===========================================================================
  # Handles:
  #   - Authentication (API keys)
  #   - Rate limiting
  #   - Request logging
  #   - Routing to vLLM
  gateway:
    build:
      context: ../../gateway
      dockerfile: Dockerfile
    container_name: llm-gateway
    environment:
      # Worker configuration
      - WORKER_TYPE=${WORKER_TYPE:-vllm}
      - VLLM_URL=http://vllm:8000
      # Fallback to echo worker if vLLM not available
      - ECHO_FALLBACK=${ECHO_FALLBACK:-false}
    volumes:
      # Persist logs
      - gateway_logs:/app/logs
      # Mount api_keys for live updates (optional)
      - ../../gateway/api_keys.json:/app/api_keys.json
    ports:
      - "8000:8000"
    networks:
      - llm-network
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    restart: unless-stopped

# Networks
networks:
  llm-network:
    driver: bridge
    name: llm-inference-network

# Volumes for persistence
volumes:
  huggingface_cache:
    name: llm-hf-cache
  gateway_logs:
    name: llm-gateway-logs
