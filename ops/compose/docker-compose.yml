# LLM Inference Stack - Docker Compose
#
# This brings up the complete inference stack:
#   - gateway: OpenAI-compatible API (port 8000)
#   - vllm: LLM inference engine (internal port 8001)
#
# Usage:
#   docker compose up              # Start with default model (Mistral 7B)
#   docker compose up -d           # Start in background
#   docker compose down            # Stop all services
#
# Switch models:
#   MODEL=google/gemma-2-9b-it docker compose up
#
# View logs:
#   docker compose logs -f vllm    # vLLM logs (model loading progress)
#   docker compose logs -f gateway # Gateway logs (requests)

services:
  # ===========================================================================
  # vLLM - LLM Inference Engine
  # ===========================================================================
  # Uses the official vLLM OpenAI-compatible image directly
  vllm:
    image: vllm/vllm-openai:latest
    container_name: llm-vllm
    # Override the default command with explicit model and args
    command:
      - "--model"
      - "${MODEL:-mistralai/Mistral-7B-Instruct-v0.3}"
      - "--max-model-len"
      - "${MAX_MODEL_LEN:-8192}"
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--trust-remote-code"
    environment:
      - HF_HOME=/root/.cache/huggingface
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    ports:
      - "8001:8000"
    networks:
      - llm-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s # Models take time to load
    restart: unless-stopped

  # ===========================================================================
  # Gateway - OpenAI-compatible API
  # ===========================================================================
  gateway:
    build:
      context: ../../gateway
      dockerfile: Dockerfile
    container_name: llm-gateway
    environment:
      - WORKER_TYPE=${WORKER_TYPE:-vllm}
      - VLLM_URL=http://vllm:8000
    volumes:
      - gateway_logs:/app/logs
      - ../../gateway/api_keys.json:/app/api_keys.json
    ports:
      - "8000:8000"
    networks:
      - llm-network
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    restart: unless-stopped

networks:
  llm-network:
    driver: bridge
    name: llm-inference-network

volumes:
  huggingface_cache:
    name: llm-hf-cache
  gateway_logs:
    name: llm-gateway-logs
